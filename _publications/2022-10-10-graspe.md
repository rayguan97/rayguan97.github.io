---
title: "GrASPE: Graph based Multimodal Fusion for Robot Navigation in Unstructured Outdoor Environments"
collection: publications
permalink: /publication/2022-10-10-graspe
# excerpt: 'We present an algorithm, Fourier Activity Recognition (FAR), for UAV video activity recognition. Our formulation uses a novel Fourier object disentanglement method to innately separate out the human agent (which is typically small) from the background. Our disentanglement technique operates in the frequency domain to characterize the extent of temporal change of spatial pixels, and exploits convolution-multiplication properties of Fourier transform to map this representation to the corresponding object-background entangled features obtained from the network. To encapsulate contextual information and long-range space-time dependencies, we present a novel Fourier Attention algorithm, which emulates the benefits of self-attention by modeling the weighted outer product in the frequency domain. Our Fourier attention formulation uses much fewer computations than self-attention. We have evaluated our approach on multiple UAV datasets including UAV Human RGB, UAV Human Night, Drone Action, and NEC Drone. We demonstrate a relative improvement of 8.02% - 38.69% in top-1 accuracy and up to 3 times faster over prior works.'
date: 2023-09-10
venue: 'IEEE Robotics and Automation Letters'
short: 'RA-L'
paperurl: 'https://arxiv.org/abs/2209.05722'
skip: "yes"
teaser: '../images/graspe_teaser.png'
authors: "Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Jing Liang, <b>Tianrui Guan</b>, Utsav Patel, Dinesh Manocha"
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
redirect_from: 
  - /graspe
---

<p style="text-align:center;">
<img src="../images/graspe_vis.png" width="900">
</p>

## Abstract

<div style="text-align: justify"> We present a novel trajectory traversability estimation and planning algorithm for robot navigation in complex outdoor environments. We incorporate multimodal sensory inputs from an RGB camera, 3D LiDAR, and the robot's odometry sensor to train a prediction model to estimate candidate trajectories' success probabilities based on partially reliable multi-modal sensor observations. We encode high-dimensional multi-modal sensory inputs to low-dimensional feature vectors using encoder networks and represent them as a connected graph. The graph is then used to train an attention-based Graph Neural Network (GNN) to predict trajectory success probabilities. We further analyze the number of features in the image (corners) and point cloud data (edges and planes) separately to quantify their reliability to augment the weights of the feature graph representation used in our GNN. During runtime, our model utilizes multi-sensor inputs to predict the success probabilities of the trajectories generated by a local planner to avoid potential collisions and failures. Our algorithm demonstrates robust predictions when one or more sensor modalities are unreliable or unavailable in complex outdoor environments. We evaluate our algorithm's navigation performance using a Spot robot in real-world outdoor environments. We observe an increase of 10-30% in terms of navigation success rate and a 13-15% decrease in false positive estimations compared to the state-of-the-art navigation methods.
</div>

<br>

<p style="text-align:center;">
<img src="../images/graspe_net.png" width="600">
</p>

## Video
<iframe width="720" height="405" src="https://www.youtube.com/embed/35hT8gokWhc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<br>
The paper is available [here](https://arxiv.org/abs/2209.05722). Please cite our work if you found it useful,

```
@ARTICLE{weerakoon2022graspe,
      title={GrASPE: Graph based Multimodal Fusion for Robot Navigation in Unstructured Outdoor Environments}, 
      author={Kasun Weerakoon and Adarsh Jagan Sathyamoorthy and Jing Liang and Tianrui Guan and Utsav Patel and Dinesh Manocha},
      year={2023},
      journal={IEEE Robotics and Automation Letters}, 
}
```
