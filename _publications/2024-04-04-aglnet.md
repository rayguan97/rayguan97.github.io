---
title: "AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales"
collection: publications
permalink: /publication/2024-04-04-aglnet
excerpt: "We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and data can be accessed at https://github.com/rayguan97/AGL-Net."
date: 2024-04-04
venue: 'The 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems'
short: 'IROS'
paperurl: 'https://arxiv.org/abs/2404.03187'
teaser: '/images/aglnet_cover.png'
authors: "<b>Tianrui Guan*</b>, Ruiqi Xian*, Xijun Wang, Xiyang Wu, Mohamed Elnoor, Daeun Song, Dinesh Manocha"
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
redirect_from: 
  - /aglnet
---

<p style="text-align:center;">
<img src="/images/aglnet_cover.png" width="800">
</p>

## Abstract
<div style="text-align: justify">We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and data can be accessed at https://github.com/rayguan97/AGL-Net.
</div>
<br>

## Video
<iframe width="720" height="405" src="https://www.youtube.com/embed/8EchVB6g3BQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

|Paper|Code| Video | 
|---|---|---|
|[**AGL-NET**](https://arxiv.org/abs/2404.03187) | [**Code**](https://github.com/rayguan97/AGL-Net)| [**Video**](https://youtu.be/8EchVB6g3BQ) |

<br>

Please cite our work if you found it useful,

```
@misc{guan2024aglnet,
      title={AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales}, 
      author={Tianrui Guan and Ruiqi Xian and Xijun Wang and Xiyang Wu and Mohamed Elnoor and Daeun Song and Dinesh Manocha},
      year={2024},
      eprint={2404.03187},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.03187}, 
}
```